---
title: "Final assingment BDAwR"
author: "Lino Zurm√ºhl"
date: "8 9 2020"
output: html_document
bibliography: TakeHomeFinal.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Article review "Are Social Bots on Twitter Political Actors? Empirical Evidence from a Ukrainian Social Botnet" Hegelich/Janetzko 2016

In their article from 2016 Hegelich and Janetzko asked themselves the question: are social bots on Twitter Political Actors? To answer this question they picked an exemplary botnet that was posting about the Ukrainian/Russian conflict. The authors collected the data via the twitter hashtag "#Ukraine" and with this dataset tried to find a political agenda behind the, in the dataset emerging, Ukrainian social botnet. Furthermore, they posted the question of what different behaviors of the botnet can be identified, if the botnet is only generating meaningless content (in the article labeled as "noise"[@hegelichAreSocialBots2016]) or political content as well, which words are used the most by the botnet and how do they correlate?  
By answering those questions Hegelich and Janetzko contribute to a better understanding of botnets in general and reveal the difficulty of actually identifying a bot. This article discloses, how much effort is put into the camouflaging of a bot relative to the actual (political) content that the bot is trying to get out. 

Hegelich and Janetzko used different types of analysis to get to this understanding of the botnet. They created a word cloud, looked at the most frequent words used by the botnet, identified the correlations between them, visualized them, and used cluster-analysis, like k-Means cluster and Hierarchical cluster analysis to identify the separation between political content and noise.

The word cloud does not give a clear answer to the question about the political agenda. There are frequent tweets of political and non-political nature to be seen in the cloud. To get a better overview the authors selected the terms that were used at least 50  times, resulting in a list of words in which political terms were heavily favored. In their next step, Hegelich and Janetzko looked at the correlation between these most frequent words and concluded that the most common pair of words was a Ukrainian party name. The k-Means Cluster-Analysis sheds light on the actual behaviors of the botnet. The analysis shows that the biggest cluster of tweets from the bots are links to news-articles, illegal film downloads, and jokes, so there is no real political agenda to be found. But in most of the other clusters, political themes are dominating. But this analysis is the most telling for the distinctive behaviors of the bots which can be summarized into three behaviors: "hid[ing] their bot-identity [...] by being interesting to normal users whilst promoting topics"[@hegelichAreSocialBots2016].

With these findings, the two authors conclude that firstly: a political agenda can be seen inside of the examined botnet, and secondly: the three patterns of behavior mentioned above (mimicry, Window Dressing, Reverberation) emerge in the examined botnet. In their last paragraph, Hegelich and Janetzko also state that the study reveals the autonomy of the bots, which in my view could be explained in more detail, as it is not clear why these findings infer which algorithms and which abstract rules the bots follow. 


```{r}
#install.packages("rvest")
#install.packages("tidyverse")
#install.packages("rtweet")
#install.packages("stringr")
#install.packages("tidytext")
#install.packages("magrittr")
#install.packages("stopwords")
#install.packages("SnowballC")
#install.packages("textdata")
#install.packages("usmap")
library(usmap)
library(textdata)
library(rvest)
library(tidytext)
library(magrittr)
library(stopwords)
library(SnowballC)
library(tidyverse)
library(stringr)
data <- read_csv("timelines.csv") %>% 
  mutate(screen_name = str_to_lower(screen_name))#twitter names case sensitive not matching the twitter data

wiki_senators <- read_html("https://en.wikipedia.org/wiki/List_of_current_United_States_senators") %>% 
  html_table(fill = TRUE) %>% 
  .[[5]] 


twitter_names <- read_csv("https://raw.githubusercontent.com/oduwsdl/US-Congress/master/116thCongress/116Congress.csv") %>% 
  select(RealName = `Wikipedia  Names`, screen_name = CSPAN) %>%
  mutate(screen_name = str_to_lower(screen_name)) #twitter names case sensitive not matching the twitter data

data_name <- data %>%
  left_join(twitter_names, by = "screen_name")


wiki_senators <- wiki_senators %>%
  select(-2,-4) %>%
  rename (RealName = Senator)


data_sen <- data_name %>%
  right_join(wiki_senators, by = "RealName")

data_selected <- data_sen %>%
  select(RealName, created_at, screen_name, type, text, favorite_count, Party, State, status_id) %>%
  rename(state=State)
  

keywords <- c("pandemic", "covid19", "virus", "corona", "lockdown", "quarantine", "fauci", "covid", "coronovirus", "masks", "mask", "testing", "CDC", "second wave")
relevant_postings <- data_selected %>% 
  filter(str_detect(text, pattern = paste(keywords, collapse = "|"))) 
  

relevant_postings_clean <- relevant_postings %>%
  mutate(text = str_to_lower(text),
         text = str_replace_all(text, "[^[:alnum:] ]", " "),
         text = str_squish(text))

relevant_postings_tidy <- relevant_postings_clean %>% 
  unnest_tokens(output = token, input = text) %>% 
  anti_join(get_stopwords(), by = c("token" = "word")) %>% 
  filter(!str_detect(token, "[:digit:]")) %>% 
  mutate(token = wordStem(token, language = "en"))

relevant_postings_tidy_word <-relevant_postings_tidy %<>% rename(word = token)

relevant_postings_sent <-
  get_sentiments("afinn") %>% 
  mutate(word = wordStem(word, language = "en")) %>%
  inner_join(relevant_postings_tidy_word)

rel_post_date <-relevant_postings_sent %>%
  filter(created_at >= as.Date('2020-06-01') & created_at <= as.Date('2020-07-01'))

rel_post_1_tweet <- rel_post_date %>%
  group_by(created_at, screen_name, type, RealName, Party, state, status_id) %>%
  summarise(totalSent = sum(value))

```
graph State
```{r}
graph_sen_state <- ggplot(data = rel_post_1_tweet) +
  geom_jitter(aes(x = created_at, y = totalSent, color = totalSent))+
  scale_color_gradient(high = "green",
                         low = "red") +
  labs(title = "Sentiment of Senators", subtitle = "Sentiment of Tweets by different States")+
  facet_wrap(~state,
               nrow = 5)

graph_sen_state
```
Graph Party

```{r}
graph_sen_party <- ggplot(data = rel_post_1_tweet) +
  geom_point(aes(x = created_at, y = totalSent, color = totalSent))+
  scale_color_gradient(high = "green",
                         low = "red") +
  facet_wrap(~Party,
               nrow = 5)

graph_sen_party
```
Us map
```{r}
plot_usmap(data = rel_post_1_tweet, values = "totalSent" ) + 
  scale_fill_continuous(
    low = "red", high = "green", name = "Sentiment", label = scales::comma
  ) + theme(legend.position = "right")
```
Us Cases
```{r}
corona_cases <- read_csv("table_corona.csv")%>%
  filter(Date >= as.Date('2020-06-01') & Date <= as.Date('2020-07-01'))

plot_usmap(data = corona_cases, values = "CasesPop" ) + 
  scale_fill_continuous( low = "white", high = "red") +
  theme(legend.position = "right")


```





References

