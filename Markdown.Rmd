---
title: "Final assingment BDAwR"
author: "Lino ZurmÃ¼hl"
date: "8 9 2020"
output:
  pdf_document: default
  html_document: default
bibliography: TakeHomeFinal.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Article review "Are Social Bots on Twitter Political Actors? Empirical Evidence from a Ukrainian Social Botnet" Hegelich/Janetzko 2016

In their article from 2016 Hegelich and Janetzko asked themselves the question: are social bots on Twitter Political Actors? To answer this question they picked an exemplary botnet that was posting about the Ukrainian/Russian conflict. The authors collected the data via the twitter hashtag "#Ukraine" and with this dataset tried to find a political agenda behind the, in the dataset emerging, Ukrainian social botnet. Furthermore, they posted the question of what different behaviors of the botnet can be identified, if the botnet is only generating meaningless content (in the article labeled as "noise"[@hegelichAreSocialBots2016]) or political content as well, which words are used the most by the botnet and how do they correlate?  
By answering those questions, Hegelich and Janetzko contribute to a better understanding of botnets in general and reveal the difficulty of actually identifying a bot. This article discloses how much effort is put into the camouflaging of a bot relative to the actual (political) content that the bot is trying to get out. 

Hegelich and Janetzko used different types of analysis to get to this understanding of the botnet. They created a word cloud, looked at the most frequent words used by the botnet, identified the correlations between them, visualized them, and used cluster-analysis, like k-Means cluster and Hierarchical cluster analysis to identify the separation between political content and noise.

The word cloud does not give a clear answer to the question about the political agenda. There are frequent tweets of political and non-political nature to be seen in the cloud. To get a better overview the authors selected the terms that were used at least 50  times, resulting in a list of words in which political terms were heavily favored. In their next step, Hegelich and Janetzko looked at the correlation between these most frequent words and concluded that the most common pair of words was a Ukrainian party name. The k-Means Cluster-Analysis sheds light on the actual behaviors of the botnet. The analysis shows that the biggest cluster of tweets from the bots are links to news-articles, illegal film downloads, and jokes, so there is no real political agenda to be found. But in most of the other clusters, political themes are dominating. But this analysis is the most telling for the distinctive behaviors of the bots which can be summarized into three behaviors: "hid[ing] their bot-identity [...] by being interesting to normal users whilst promoting topics"[@hegelichAreSocialBots2016].

With these findings, the two authors conclude that firstly: a political agenda can be seen inside of the examined botnet, and secondly: the three patterns of behavior mentioned above (mimicry, Window Dressing, Reverberation) emerge in the examined botnet. In their last paragraph, Hegelich and Janetzko also state that the study reveals the autonomy of the bots, which in my view could be explained in more detail, as it is not clear why these findings infer which algorithms and which abstract rules the bots follow. The second point of criticism would be the Circle-Dendogram of their hierarchical cluster analysis, which is not a very telling graph. There is just too much going on in this figure, it is not lucid. 

The Data analyzed by the two authors can be called "Big Data" if we use Matthew Salaganik's definition [zitat] of big data. Hegelich and Janetzko took all the tweets of 1700 accounts they identified as bots in a 24-hour timespan, which ticks the boxes of "big", "always-on" and "nonreactive". Twitter data in general seems to match the common features of big data due to the means the data is generated: via a social media platform. 

Because the problem of social botnets is a rather new one, this study could be seen as basic research about the behaviors of these botnets, even tho the paper only examines one specific example. By extracting the big data set from twitter and identifying the bots due to their URL, it enabled the authors to get such a great number of relevant data points, that they were able to get a good understanding of social botnets. 
Hegelich and Janetzko were cautious to filter out the "noise" from the real content, which shows good awareness of the dirtiness of their dataset. They did not base their research only on the word cloud they looked further into the data to pull the curtain away, that the "noise" has generated. -> algorithmically confounded



### Data challange

## Introduction

If there is one big topic discussed in American politics right now, it is the Covid-19 pandemic. .....
This article tries to address the following research question: Is there a connection between a high number of Covid cases in a state and negative tweets about the coronavirus posted by the Senators of this particular state? To get an answer to this question I used sentiment analysis on the tweets from 01.06.2020 until 01.07.2020 from senators to get an estimated sentiment of each tweet. This sentiment was then matched with data of the corona cases in each state. The result demonstrates that there are too many variables that play into the tweet behavior of senators to recognize a high number of corona cases as the cause of a negative tweets about the pandemic. 

## The Data

The data used in this article are the tweets from every senator in the month of June. I made this choice to get a particular timeframe due to the ever changing number of Covid-19 cases.  These tweets were matched with data from Wikipedia in order to obtain the states they represent and after that filtered by keywords associated with the pandemic. The data on the Corona cases I got from my fellow classmate Lena Richter, who worked on the Corona dataset. It contains Covid cases of each state per 100000 inhabitants. 






```{r, echo=FALSE, message=FALSE, warning=FALSE}
#install.packages("rvest")
#install.packages("tidyverse")
#install.packages("rtweet")
#install.packages("stringr")
#install.packages("tidytext")
#install.packages("magrittr")
#install.packages("stopwords")
#install.packages("SnowballC")
#install.packages("textdata")
#install.packages("usmap")
#install.packages("wordcloud")
#install.packages("RColorBrewer")
#install.packages("tm")
#install.packages("lm.beta")
#install.packages("corrr")
#install.packages("moderndive")
library(moderndive)
library(corrr)
library(lm.beta)
library(usmap)
library(textdata)
library(rvest)
library(tidytext)
library(magrittr)
library(stopwords)
library(SnowballC)
library(tidyverse)
library(stringr)
library(wordcloud)
library(RColorBrewer)
library(tm)
data <- read_csv("timelines.csv") %>% 
  mutate(screen_name = str_to_lower(screen_name))#twitter names case sensitive not matching the twitter data

wiki_senators <- read_html("https://en.wikipedia.org/wiki/List_of_current_United_States_senators") %>% 
  html_table(fill = TRUE) %>% 
  .[[5]] %>%
  select(-2,-4) %>%
  rename (RealName = Senator)


twitter_names <- read_csv("https://raw.githubusercontent.com/oduwsdl/US-Congress/master/116thCongress/116Congress.csv") %>% 
  select(RealName = `Wikipedia  Names`, screen_name = CSPAN) %>%
  mutate(screen_name = str_to_lower(screen_name)) #twitter names case sensitive not matching the twitter data

data_name <- data %>%
  left_join(twitter_names, by = "screen_name") #getting the real names of the senetors and representatives

data_sen <- data_name %>%
  right_join(wiki_senators, by = "RealName")# joining only the tweets of the senators

data_selected <- data_sen %>%
  select(RealName, created_at, screen_name, type, text, favorite_count, Party, State, status_id) %>%
  rename(state=State) #only the variables that are needed
  

keywords <- c("pandemic", "covid19", "virus", "corona", "lockdown", "quarantine", "fauci", "covid", "coronovirus", "masks", "mask", "testing", "CDC", "second wave")
relevant_postings <- data_selected %>% 
  filter(str_detect(text, pattern = paste(keywords, collapse = "|"))) #getting only the tweets with corona content
  

relevant_postings_clean <- relevant_postings %>%
  mutate(text = str_to_lower(text),
         text = str_replace_all(text, "[^[:alnum:] ]", " "),
         text = str_squish(text))#preprocessing

relevant_postings_tidy <- relevant_postings_clean %>% 
  unnest_tokens(output = token, input = text) %>% 
  anti_join(get_stopwords(), by = c("token" = "word")) %>% 
  filter(!str_detect(token, "[:digit:]")) %>% 
  mutate(token = wordStem(token, language = "en"))#tidytext



relevant_postings_tidy_word <-relevant_postings_tidy %<>% rename(word = token)


relevant_postings_sent <-
  get_sentiments("afinn") %>% 
  mutate(word = wordStem(word, language = "en")) %>%
  inner_join(relevant_postings_tidy_word)# getting only words with sentiment

rel_post_date <-relevant_postings_sent %>%
  filter(created_at >= as.Date('2020-06-01') & created_at <= as.Date('2020-07-01'))#using the 1 month timespan



rel_post_1_tweet <- rel_post_date %>%
  group_by(created_at, screen_name, type, RealName, Party, state, status_id) %>%
  summarise(totalSent = sum(value))# getting the sentiment of each individual tweet

```



## Wordcloud

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Removing the stemming function to get full words in the wordcloud
relevant_postings_no_stem <- relevant_postings_clean %>% 
  unnest_tokens(output = token, input = text) %>% 
  anti_join(get_stopwords(), by = c("token" = "word")) %>% 
  filter(!str_detect(token, "[:digit:]"))%>% 
  rename(word = token)


relevant_postings_wordcloud <- 
  get_sentiments("afinn") %>% 
  inner_join(relevant_postings_no_stem)


wordcloud(words = relevant_postings_wordcloud$word, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "RdYlGn"))

  

```







## Graph State

```{r, echo=FALSE, fig.height=7, fig.width=15, message=FALSE, warning=FALSE}
graph_sen_state <- ggplot(data = rel_post_1_tweet) +
  geom_jitter(aes(x = created_at, y = totalSent, color = totalSent))+
  scale_color_gradient(high = "green",
                         low = "red") +
  labs(title = "Sentiment of Senators", subtitle = "Sentiment of Tweets by different States")+
 theme_dark()+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
  ylab("Sentiment")+
  facet_wrap(~state,
               nrow = 5)

graph_sen_state
```


##  Us map

```{r, echo=FALSE, fig.show="hold", out.width="50%", message=FALSE, warning=FALSE}
corona_cases <- read_csv("table_corona.csv")%>%
  filter(Date == as.Date('2020-07-01')) #this data I obtained from my fellow course participant Lena Richter, who worked on the corona dataset

plot_usmap(data = rel_post_1_tweet, values = "totalSent" ) + 
  scale_fill_continuous(
    low = "red", high = "white", name = "Sentiment", label = scales::comma
  ) + theme(legend.position = "right")+
  labs(title = "Sentiment of Senators", subtitle = "Sentiment of Tweets from the Senators of each State")+


plot_usmap(data = corona_cases, values = "CasesPop" ) + 
  scale_fill_continuous( low = "white", high = "red", name = "Cases per 100k inhabitants", limits=c(0,2500)) +
  theme(legend.position = "right")
```




## Us Map Party





```{r, echo=FALSE, message=FALSE, warning=FALSE}
graph_sen_party <- ggplot(data = rel_post_1_tweet) +
  geom_point(aes(x = created_at, y = totalSent, color = totalSent))+
  scale_color_gradient(high = "green",
                         low = "red", name = "Sentiment") +
   labs(title = "Sentiment of the Party", subtitle = "Sentiment of Tweets by different Parties")+
  theme_dark()+
  xlab("Date")+
  ylab("Sentiment")+
  facet_wrap(~Party,
               nrow = 5)

graph_sen_party



```





References

