---
title: "Final assingment BDAwR"
author: "Lino Zurm√ºhl"
date: "`r lubridate::today()`"
output:
  pdf_document: default
  html_document: default
bibliography: TakeHomeFinal.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Article review "Are Social Bots on Twitter Political Actors? Empirical Evidence from a Ukrainian Social Botnet" Hegelich/Janetzko 2016

In their article from 2016 Hegelich and Janetzko asked themselves the question: are social bots on Twitter Political Actors? To answer this question they picked an exemplary botnet that was posting about the Ukrainian/Russian conflict. The authors collected the data via the twitter hashtag "#Ukraine" and with this dataset tried to find a political agenda behind the, in the dataset emerging, Ukrainian social botnet. Furthermore, they posted the question of what different behaviors of the botnet can be identified, if the botnet is only generating meaningless content (in the article labeled as "noise"[@hegelichAreSocialBots2016]) or political content as well, which words are used the most by the botnet and how do they correlate?  
By answering those questions, Hegelich and Janetzko contribute to a better understanding of botnets in general and reveal the difficulty of actually identifying a bot. This article discloses how much effort is put into the camouflaging of a bot relative to the actual (political) content that the bot is trying to get out. 

Hegelich and Janetzko used different types of analysis to get to this understanding of the botnet. They created a word cloud, looked at the most frequent words used by the botnet, identified the correlations between them, visualized them, and used cluster-analysis, like k-Means cluster and Hierarchical cluster analysis to identify the separation between political content and noise.

The word cloud does not give a clear answer to the question about the political agenda. There are frequent tweets of political and non-political nature to be seen in the cloud. To get a better overview the authors selected the terms that were used at least 50  times, resulting in a list of words in which political terms were heavily favored. In their next step, Hegelich and Janetzko looked at the correlation between these most frequent words and concluded that the most common pair of words was a Ukrainian party name. The k-Means Cluster-Analysis sheds light on the actual behaviors of the botnet. The analysis shows that the biggest cluster of tweets from the bots are links to news-articles, illegal film downloads, and jokes, so there is no real political agenda to be found. But in most of the other clusters, political themes are dominating. But this analysis is the most telling for the distinctive behaviors of the bots which can be summarized into three behaviors: "hid[ing] their bot-identity [...] by being interesting to normal users whilst promoting topics"[@hegelichAreSocialBots2016].

With these findings, the two authors conclude that firstly: a political agenda can be seen inside of the examined botnet, and secondly: the three patterns of behavior mentioned above (mimicry, Window Dressing, Reverberation) emerge in the examined botnet. In their last paragraph, Hegelich and Janetzko also state that the study reveals the autonomy of the bots, which in my view could be explained in more detail, as it is not clear why these findings infer which algorithms and which abstract rules the bots follow. The second point of criticism would be the Circle-Dendogram of their hierarchical cluster analysis, which is not a very telling graph. There is just too much going on in this figure, it is not lucid. 

The Data analyzed by the two authors can be called "Big Data" if we use Matthew Salaganik's definition [@salganikBitBitSocial2018] of big data. Hegelich and Janetzko took all the tweets of 1700 accounts they identified as bots in a 24-hour timespan, which ticks the boxes of "big", "always-on" and "nonreactive". Twitter data in general seems to match the common features of big data due to the means the data is generated: via a social media platform. 

Because the problem of social botnets is a rather new one, this study could be seen as basic research about the behaviors of these botnets, even tho the paper only examines one specific example. By extracting the big data set from twitter and identifying the bots due to their URL, it enabled the authors to get such a great number of relevant data points, that they were able to get a good understanding of social botnets. 
Hegelich and Janetzko were cautious to filter out the "noise" from the real content, which shows good awareness of the dirtiness of their dataset. They did not base their research only on the word cloud they looked further into the data to pull the curtain away, that the "noise" has generated. 



## Data challenge

### Introduction

If there is one big topic discussed in American politics right now, it is the Covid-19 pandemic. How are politicians communicating this crisis with the people?
This article tries to address the following research question: Is there a connection between a high number of COVID cases in a state and negative tweets about the coronavirus posted by the Senators of this particular state? To get an answer to this question I used sentiment analysis on the tweets from 01.06.2020 until 01.07.2020 from senators to get an estimated sentiment of each tweet. This sentiment was then matched with data of the corona cases in each state. The result demonstrates that there are too many variables that play into the tweeting behavior of senators to recognize a high number of corona cases as the cause of negative tweets about the pandemic. 

### The Data

The data used in this article are the tweets from every senator in June. I made this choice to get a particular timeframe due to the ever-changing number of Covid-19 cases.  These tweets were matched with data from Wikipedia to obtain the states they represent and after that filtered by keywords associated with the pandemic. The data on the Corona cases I got from my classmate Lena Richter, who worked on the Corona dataset. It contains COVID cases of each state per 100000 inhabitants. 

### Methods

To get the dataset ready for the analysis, only the necessary variables were filtered out for a better overview of the data. Secondly, the raw text of the tweets had to be pre-processed and tidied up for the sentiment analysis to work. That means all text to lower case, removing stopwords and stemming each word. This resulted in a dataset in which every stemmed word of every tweet occurs individually so that it is possible to join this data with the data containing a list of stemmed words with a sentiment from the tidytext package. The final step involved regrouping the data from individual words to the whole tweet and summing up the sentiments of each individual word in one tweet, in order to get the sentiment of each tweet. With this done, I used the word cloud package to generate a cloud of the most used words with a sentiment in the tweets (*figure 1*) and a boxplot with words used at least 300 times(*figure 2*). To get a better overview of the tweet sentiment in the individual states *figure 3* was plotted, but this graph does not give any information about the connection to the corona cases. To archive this I created 2 heatmaps (*figures 4/5*) with the "usmap" package, one illustrating the sentiment of the senators from each state and the other showing the Covid-19 cases. The last plot displays another possible variable that might influence the sentiment of the tweets: the partisanship of each senator. 


### The Results
```{r, echo=FALSE, message=FALSE, warning=FALSE}
#install.packages("rvest")
#install.packages("tidyverse")
#install.packages("rtweet")
#install.packages("stringr")
#install.packages("tidytext")
#install.packages("magrittr")
#install.packages("stopwords")
#install.packages("SnowballC")
#install.packages("textdata")
#install.packages("usmap")
#install.packages("wordcloud")
#install.packages("RColorBrewer")
#install.packages("tm")
library(usmap)
library(textdata)
library(rvest)
library(tidytext)
library(magrittr)
library(stopwords)
library(SnowballC)
library(tidyverse)
library(stringr)
library(wordcloud)
library(RColorBrewer)
library(tm)
data <- read_csv("timelines.csv") %>% 
  mutate(screen_name = str_to_lower(screen_name))#twitter names case sensitive not matching the twitter data

wiki_senators <- read_html("https://en.wikipedia.org/wiki/List_of_current_United_States_senators") %>% 
  html_table(fill = TRUE) %>% 
  .[[5]] %>%
  select(-2,-4) %>%
  rename (RealName = Senator)

#write_csv(wiki_senators, "Bigdata/TakeHomeFinal")


twitter_names <- read_csv("https://raw.githubusercontent.com/oduwsdl/US-Congress/master/116thCongress/116Congress.csv") %>% 
  select(RealName = `Wikipedia  Names`, screen_name = CSPAN) %>%
  mutate(screen_name = str_to_lower(screen_name)) #twitter names case sensitive not matching the twitter data

data_name <- data %>%
  left_join(twitter_names, by = "screen_name") #getting the real names of the senetors and representatives

data_sen <- data_name %>%
  right_join(wiki_senators, by = "RealName")# joining only the tweets of the senators

data_selected <- data_sen %>%
  select(RealName, created_at, screen_name, type, text, favorite_count, Party, State, status_id) %>%
  rename(state=State) #only the variables that are needed
  

keywords <- c("pandemic", "covid19", "virus", "corona", "lockdown", "quarantine", "fauci", "covid", "coronovirus", "masks", "mask", "testing", "CDC", "second wave")
relevant_postings <- data_selected %>% 
  filter(str_detect(text, pattern = paste(keywords, collapse = "|"))) #getting only the tweets with corona content
  

relevant_postings_clean <- relevant_postings %>%
  mutate(text = str_to_lower(text),
         text = str_replace_all(text, "[^[:alnum:] ]", " "),
         text = str_squish(text))#preprocessing

relevant_postings_tidy <- relevant_postings_clean %>% 
  unnest_tokens(output = token, input = text) %>% 
  anti_join(get_stopwords(), by = c("token" = "word")) %>% 
  filter(!str_detect(token, "[:digit:]")) %>% 
  mutate(token = wordStem(token, language = "en"))#tidytext



relevant_postings_tidy_word <-relevant_postings_tidy %<>% rename(word = token)


relevant_postings_sent <-
  get_sentiments("afinn") %>% 
  mutate(word = wordStem(word, language = "en")) %>%
  inner_join(relevant_postings_tidy_word)# getting only words with sentiment

rel_post_date <-relevant_postings_sent %>%
  filter(created_at >= as.Date('2020-06-01') & created_at <= as.Date('2020-07-01'))#using the 1 month timespan



rel_post_1_tweet <- rel_post_date %>%
  group_by(created_at, screen_name, type, RealName, Party, state, status_id) %>%
  summarise(totalSent = sum(value))# getting the sentiment of each individual tweet

```

```{r, echo=FALSE, message=FALSE, warning=FALSE , fig.height=4, fig.width=4}
#Removing the stemming function to get full words in the wordcloud
relevant_postings_no_stem <- relevant_postings_clean %>% 
  unnest_tokens(output = token, input = text) %>% 
  anti_join(get_stopwords(), by = c("token" = "word")) %>% 
  filter(!str_detect(token, "[:digit:]"))%>% 
  rename(word = token)


relevant_postings_wordcloud <- 
  get_sentiments("afinn") %>% 
  inner_join(relevant_postings_no_stem)


wordcloud(words = relevant_postings_wordcloud$word, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "RdYlGn"))

  

```
*Fig. 1. WordCloud*

The word cloud above shows the 200 most frequently tweeted words by the senators with either a positive or negative sentiment. The two most tweeted sentimental words "help" and "care" have both a positive association, which could indicate an overall positive leaning in the tweets about Covid-19. In Fig. 2, showing the words tweeted at least 300 times, we can also see the high frequency of these two words, yet here the sentiment of the overall words seems to be more mixed. 


```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=10}
relevant_postings_no_stem_count <- relevant_postings_wordcloud %>%
  count(word) %>%
  filter (n>300)

graph_freq_words <- ggplot(data= relevant_postings_no_stem_count, aes(word, n))+
  geom_bar(stat = "identity", fill= "white", coulour = "white")+
   theme_dark()+
  ylab("Frequency")+
  xlab("")+
  labs(title = "Frequency of sentimental Words", subtitle = "Word with a frequency of at least 300")+
  coord_flip()

graph_freq_words

```
*Fig. 2 Frequency of words*

In the graph below the sentiment of tweets is split up into the different states the senators represent. It can be observed that there is a big gap in the rate in which the senators are tweeting. The Senators of Georgia and Nebraska for example are tweeting far less often about Covid-19, than the senators of Nevada and Texas. The sentiment yet seems to hover on average around zero. There is no significant pattern to be seen here. 



```{r, echo=FALSE, fig.height=8, fig.width=15, message=FALSE, warning=FALSE}
graph_sen_state <- ggplot(data = rel_post_1_tweet) +
  geom_jitter(aes(x = created_at, y = totalSent, color = totalSent))+
  scale_color_gradient(high = "green",
                         low = "red", name = "Sentiment") +
  labs(title = "Sentiment of Senators", subtitle = "Sentiment of Tweets by different States")+
 theme_dark()+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
  ylab("Sentiment")+
  facet_wrap(~state,
               nrow = 5)

graph_sen_state
```
*Fig. 3: Sentiment of senators*

By plotting the sentiment data in a heatmap we can compare them to the number of Covid-19 in each state. Here (*Fig. 4/5*) we can observe that even tho states like New York have a lot of COVID cases, the sentiment of tweets posted by the New Yorker senators is not as negative. Again we cannot identify a significant pattern in this figure. (Florida is gray, due to insufficient twitter data)



```{r, echo=FALSE, fig.show="hold", out.width="50%", message=FALSE, warning=FALSE}
corona_cases <- read_csv("table_corona.csv")%>%
  filter(Date == as.Date('2020-07-01')) #this data I obtained from my fellow course participant Lena Richter, who worked on the corona dataset

plot_usmap(data = rel_post_1_tweet, values = "totalSent" ) + 
  scale_fill_continuous(
    low = "red", high = "white", name = "Sentiment", label = scales::comma
  ) + theme(legend.position = "right")+
  labs(title = "Sentiment of Senators", subtitle = "Sentiment of Tweets from the Senators of each State")


plot_usmap(data = corona_cases, values = "CasesPop" ) + 
  scale_fill_continuous( low = "white", high = "red", name = "Cases per 100k inhabitants", limits=c(0,2500)) +
  theme(legend.position = "right")+
  labs(title = "Cases of Covid-19 ", subtitle = "Cases of Covid-19 on 01.07.2020 per 100k  inhabitants")
```
*Fig. 4/5 Connection between sentiment and Covid-19 cases*

All these graphs disclose that there are problems with the hypothesis of this article. None of them really could illustrate a correlation between a negative sentiment of senators tweets and a high number of corona cases in each state. The first thing that may blur the correlation is the fact that most senators do not aspire to cause panic in the population, therefore they use a more positive tone to describe the current situation. Secondly, there are just too many variables that play into these tweets. Partisanship for example could be a big variable that causes the senators to tweet in a specific manner.  

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
graph_sen_party <- ggplot(data = rel_post_1_tweet) +
  geom_point(aes(x = created_at, y = totalSent, color = totalSent))+
  scale_color_gradient(high = "green",
                         low = "red", name = "Sentiment") +
   labs(title = "Sentiment of the Party", subtitle = "Sentiment of Tweets by different Parties")+
  theme_dark()+
  xlab("Date")+
  ylab("Sentiment")+
  facet_wrap(~Party,
               nrow = 5)

graph_sen_party



```
*Fig. 6 Partisanship*

In *Fig. 6* the difference between the democratic and republican Party can be observed. The Republican Senators seem to use more positive words in their tweets about Covid-19 than Democrats. This is just one example of the different variables that play a role in these tweets. 

And lastly, the data seems not to be big enough. The twitter data of some senators, like the senators of Florida, were so marginal they could not be taken into account. A cause of this could be the age of the senators, hence they don't use social media that frequently.



*References*

